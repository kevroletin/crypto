\documentclass[10pt,a4paper]{article}
\usepackage[cm]{fullpage}
\usepackage[utf8x]{inputenc}
\usepackage{ucs}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage[russian]{babel}
\usepackage{listings}
\author{Кевролетин В.В.}
\title{Применение теории информации}
\begin{document}

\maketitle

\subsection*{Задание4.1}
\subsubsection*{Условие}
В алфавите исходного и зашифрованного сообщения имеется 6 гласных и 6
согласных букв. Все согласные передаются без искажения. Гласная в 50\%
случаев передается без искажений, а в 50\% равновероятно появление
любой гласной буквы. Сколько информации содержится в полученном
символе о переданном?
\subsubsection*{Решение}
Количество пришедшей информации естественно измерять величиной
исчезнувшей неопределенности [Р.Л. Стратович "Теория информации"
стр. 12].
Посчитаем энтропию до и после передачи буквы.
До передачи, т.к. алфавит состоит из 12 букв и все буквы считаем
равновероятными:
$$ H_1 = log_2(12) $$ \\
После передачи имеется 2 варианта: \\
A - символ передан без искажения(если передана гласная буква, или
согласная,н о без искажения \\
$$ P(A) = 0.5 + 0.5*0.5 $$ \\
B - символ передан с искажением(согласная с искажением)\\
$$ P(B) = 0.5 + 0.5 $$ \\
Вычислим энтропию:
$$ H_2 = - (0.75*ln(0.75) + 0.25*ln(0.25)) $$ \\
Собираем всё вместе и получаем количество информации: \\
$$ log_2(12) - (0.75*ln(0.75) + 0.25*ln(0.25)) = $$
$$ = 3.58496250072116 - 0.811278124459133 = 2.77368437626202 $$

\subsection*{Задание4.2}
\subsubsection*{Условие}
Используя понятие энтропии и информации по Шеннону, оценить
минимальное число вопросов, которое необходимо задать, чтобы
гарантированно определить задуманное собеседником число <= N, если
он дает только двоичные ответы на вопросы (да/нет).
\subsubsection*{Решение}
Минимальное число вопросов, в нашем случае, равно количеству
информации по Шеннону(т.е. минимальное кол-во бит, необходимое для
кодирования сообщения). По определению получаем:
$$ I = log_2(N) $$

\subsection*{Задание4.3}
\subsubsection*{Условие}
Имеется N монет одного достоинства. Одна из них, фальшивая, либо
легче, либо тяжелее остальных. Используя понятие энтропии и информации
по Шеннону, оценить минимальное число взвешиваний на чашечных весах
без гирь, необходимых для гарантированного нахождения фальшивой монеты
и определения, является ли она легче или тяжелее остальных.
\subsubsection*{Решение}
Имеем 2 неизвестных факта:
"одна из N монет фальшивая" и "фальшивая монета легче или
тяжелее". Общее количество информации будет равно сумме
информаций для 1го и 2го неизвестного факта(т.к. они независимы).
Для определения пары монет, отличающихся по весу необходимо $ log_2(n) $
взвешиваний. Еще одно взвешивание необходимо для определения какая
именно из монет фальшивая и тяжелее или легче она остальных.

\subsection*{Задание4.4}
\subsubsection*{Условие}
Какая информация будет получена в результате проведения зачета, если
студент получает зачет с вероятностью 0.9, если он готовился, и 0.3,
если нет, и известно, что 90\% студентов готовились к зачету.
\subsubsection*{Решение}
Получаем 4 события с соответствующими вероятностями их
возникновения:\\
Готовился и получил - 0.9 * 0.9 \\
Готовился и не получил - 0.9 * 0.1 \\
Не готовился и получил - 0.1 * 0.3 \\
Не готовился и не получил - 0.1 * 0.7 \\

$$ H = -( 0.81*log_2(0.81) + 0.03*log_2(0.03) + 0.09*log_2(0.09) +
0.07*log_2(0.07) ) = 0.979220717742703 $$

\end{document}
